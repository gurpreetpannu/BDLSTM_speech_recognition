{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random as rn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import signal\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, TimeDistributed, Dropout, Bidirectional, Activation, CuDNNLSTM, Flatten, Permute, Conv2D\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, Concatenate, Permute, Reshape, Flatten, Lambda, Dot, Softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kapre.utils import Normalization2D\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/wav/'\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "train_dict = df_train.set_index('path')['word'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function extracts mfcc, delta and delta delta features from .wav files and pads them to the standard shape of (60,44). It then stacks the features to a 3d array and also outputs two dictionary with paths as keys and values as labels which are none for the test dictionary created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMFCCfeatures():\n",
    "    train_mfcc = []\n",
    "    train_y = {}\n",
    "    test_mfcc = []\n",
    "    test_y = {}\n",
    "    pad2d = lambda a, i: a[:, 0: i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0],i - a.shape[1]))))\n",
    "    for fname in tqdm(os.listdir(DATA_DIR)[:100], desc='dir'):\n",
    "        try:\n",
    "            if '.wav' not in fname or 'dima' in fname:\n",
    "                continue\n",
    "            label = train_dict.get(fname)\n",
    "            wav, sr = librosa.load(DATA_DIR + fname)\n",
    "            mfcc = librosa.feature.mfcc(wav)\n",
    "            mfcc1 = librosa.feature.delta(mfcc)\n",
    "            mfcc2 = librosa.feature.delta(mfcc, order=2)\n",
    "            fmfcc = np.vstack((mfcc, mfcc1, mfcc2))\n",
    "            padded_mfcc = pad2d(fmfcc, 44)\n",
    "\n",
    "            if label == None:\n",
    "                test_mfcc.append(padded_mfcc)\n",
    "                test_y[fname] = label\n",
    "            else:\n",
    "                train_mfcc.append(padded_mfcc)\n",
    "                train_y[fname] = label\n",
    "        except Exception as e:\n",
    "            print (fname, e)\n",
    "            raise\n",
    "\n",
    "    return np.array(train_mfcc), np.array(test_mfcc), train_y, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function preprocesses the data and the steps included are:\n",
    "1. Encode the y labels to a one-hot encoded vector\n",
    "2. Resample the data to add more examples of each label and overall ratio of majority to minority as 1.\n",
    "3. Then the function reshapes the array to output a 4D array with an extra dimesnion added so that it can be fed to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPreProcessedData(train_, test_, y_train):\n",
    "    encoder = LabelEncoder()\n",
    "    l1 = np.array(list(y_train.values()))\n",
    "    encoder.fit(l1)\n",
    "    encoded_Y = encoder.transform(l1)\n",
    "    mfcc2d = train_.reshape(train_.shape[0], train_.shape[1]*train_.shape[2])\n",
    "    ros = RandomOverSampler(sampling_strategy='all',random_state=2019)\n",
    "    mfcc_resampled, y_resampled = ros.fit_resample(mfcc2d, encoded_Y)\n",
    "    mfcc_ori = np.reshape(mfcc_resampled, (-1, mfcc_raw.shape[1], mfcc_raw.shape[2]))\n",
    "    \n",
    "    X_train = mfcc_ori.reshape(mfcc_ori.shape[0], mfcc_ori.shape[1], mfcc_ori.shape[2], 1)\n",
    "    X_test  = test_.reshape(test_.shape[0], test_.shape[1], test_.shape[2], 1)\n",
    "    y_cat = np_utils.to_categorical(y_resampled)\n",
    "    \n",
    "    return X_train, X_test, y_cat, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined as below with 3 convolution layers, interspersed with Dropouts layer and then the output is fed into BDLSTM layers and finally condensed with a time distributed layer. This is then flattened and passed into two dense layers and finally outputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineModel(input_):\n",
    "    nclass=35\n",
    "    x = Normalization2D(int_axis=0)(input_)\n",
    "    x = Permute((2,1,3)) (x)\n",
    "\n",
    "    x = Conv2D(10, (4,1) , activation='relu', padding='same', \n",
    "             kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)) (x)\n",
    "\n",
    "    x = Dropout(rate=0.4)(x)\n",
    "    x = Conv2D(10, (4,1) , activation='relu', padding='same',\n",
    "             kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)) (x)\n",
    "\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = Conv2D(1, (4,1) , activation='relu', padding='same', \n",
    "             kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)) (x)\n",
    "\n",
    "    x = Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim') (x) \n",
    "\n",
    "    x = Bidirectional(CuDNNLSTM(512, return_sequences = True)) (x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(512, return_sequences = True)) (x)\n",
    "\n",
    "    x = TimeDistributed(Dense(1024)) (x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation = 'sigmoid')(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = Dense(128)(x)\n",
    "\n",
    "\n",
    "    output = Dense(nclass, activation = 'softmax', name='output')(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This labels predicts the model outpur on the test set and then inverse transforms them into the labels given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYLabels(testfeat, encoder):\n",
    "    y_predict = model.predict(testfeat)\n",
    "    y_enco    = np.argmax(y_predict, axis=1)\n",
    "    y_classes = encoder.inverse_transform(y_enco)\n",
    "    return y_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test path index created earlier will not be in the same order as the test paths given so this function gets the code for the paths which can be used to rearrange the output labels in correspondence with the test paths as provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mfcc, test_mfcc, y_train, y_test = getMFCCfeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xtrain, test_feat, ytrain, encoder = getPreProcessedData(train_mfcc, test_mfcc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined, compiled and fitted below. We use two keras callbacks of earlystopping and reducing LR when it plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain, test_size=0.05, random_state=2019)\n",
    "input_shape = X[0].shape\n",
    "inp = Input(shape=input_shape)\n",
    "output = defineModel(inp)\n",
    "model = Model(inputs=inp, outputs=[output])\n",
    "opt = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.99, amsgrad=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, min_lr=0.001)\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, batch_size=128, validation_split=0.1, \n",
    "        epochs=100, shuffle=True, verbose=1, callbacks=[reduce_lr, es])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
